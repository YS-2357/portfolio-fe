---

- **프로젝트명**: RFP 요약·질의응답 시스템 (RFP Summarizer & QA Chatbot)
- **팀명**: B2G 입찰 지원 전문 컨설팅 스타트업 – 'RFPilot'
- **기간**: 2025.05.12 ~ 2025.05.30
- **작성자**: 정영선, 구극모, 박규리, 이학진, 정재의
- **GitHub 링크**: https://github.com/gyurili/2025-LLM-Project

---

# 👥 팀 구성 및 역할

| **이름** | **역할** | **담당 업무** |
| --- | --- | --- |
| **정영선 (팀장)** | Def Programming / System Arch | 전체 프로젝트 구조 설계 및 코드 표준 수립; 문제 정의 및 문서 처리 파이프라인 설계; 팀원 코드 리뷰 및 디버깅 지원; 실행/개발 환경 최적화; 기술 문서(README 등) 작성 주도 |
| **구극모** | Frontend Dev / Data Engineer | Streamlit UI/UX 설계 및 개발; 사용자 인터랙션 처리; HWP 전처리(HWPLoader); Chunk 정제(Text clean, Chunk size); Embedding 및 Vector DB(FAISS, Chroma) |
| **박규리** | Backend Dev / RAG Engineer | Retriever 구성 및 Hybrid Search 실험; 질의어-청크 유사도 기반 Rerank 로직 구현; OpenAI Generator 구성; Streamlit과 FastAPI 연동; GCP VM에서 gunicorn 기반 배포 |
| **이학진** | NLP Engineer / Prompt Engineer | PDF 전처리 및 데이터 로더 통합; 이미지 기반 문서 텍스트 추출; Generator 구성 및 프롬프트 엔지니어링; LangSmith trace 로깅 설정; 문서 요약/질문 응답 품질 개선 |
| **정재의** | Frontend Dev / Data Processing | Streamlit UI/UX 설계 및 개발; 사용자 인터랙션 처리(문서 업로드, 채팅 인터페이스 등); 챗봇 반응형 UI 구현; 문서 분할(splitter) 전략 설계 및 의미론적 분할 구현; Top-k 유사 문서 필터링 로직 설계 |

# 🛠️ **주요 기술 및 라이브러리**

[🛠️ 주요 기술 및 라이브러리](https://www.notion.so/20601c050cec806ea83dfef8faa4242d?pvs=21)

# **1.** 🧾 **프로젝트 개요**

- **프로젝트 동기 및 배경**
    - 기업 및 공공기관은 다양한 형식의 제안요청서(RFP)를 **주기적으로 작성하고 검토**해야 하며, 이는 **상당한 시간과 인력을 소모**함
    - **기존 문서 검색 및 분석 방식**은 수작업 위주로, **정보 접근성**과 **업무 효율성**이 크게 떨어짐
    - 이에 따라, **RAG(Retrieval-Augmented Generation)** 기술과 **LangChain 프레임워크**를 활용해 RFP 문서를 **자동 요약**하고 **질의응답 기반으로 탐색할 수 있는 서비스**를 구현하고자 함
- **목표**
    - PDF/HWP 형식의 기업 및 정부 기관의 제안요청서(RFP) **문서를 자동으로 분할·요약**하고, **실시간 질의응답**이 가능한 **웹 기반 챗봇 서비스** 구축
    - 사용자의 질문에 대해 관련 문서 내용을 기반으로 **정확하고 출처 기반의 응답** 제공
- **기대 효과**
    - RFP 문서의 **핵심 정보를 빠르게 파악**하고 **필요한 정보를 즉시 획득** 가능
    - 반복적이고 수동적인 **문서 분석 업무 자동화** → **업무 생산성 향상**
    - **사용자 친화적 UI** 기반 문서 기반 QA 지원

---

# **2.** ⚙️ **주요 기능**

- **문서 정보**: PDF 및 HWP 포맷으로 구성된 100개의 RFP 문서
- **문서 요약**: 문서 전체 내용에 대한 핵심 요약 제공
- **질의응답**: DB에 저장된 문서 내용을 기반으로 사용자의 자연어 질문에 답변 제공
- **설정 관리**: `config.yaml`을 통한 파이프라인 전반 설정 일괄 관리
- **인터페이스**: Streamlit 기반 GUI 제공
    
    🔎 자세한 화면 구성 및 사용자 흐름은 **4.🖱️ UI 및 사용자 경험** 참조
    

---

# **3.** 🖥️ **시스템 아키텍처**
![report1](/asset/images/projects/codeit/rfp-rag/star1.png)
> RFPilot Architecture Flowchart
> 

## 3-1. Document Processing

### **i. Document Loader**

> - RFP 문서를 불러오고 필요한 **메타데이터(문서명, 기관명, 사업명 등)** 정리
- 사용자의 질문 및 메타데이터 기반으로 유사 문서를 선택하고 추출하는 **역할 수행**
- OCR이 필요한 이미지 기반 문서(PDF 스캔본 등)도 처리 가능
> 

**PDF 문서 내 텍스트 추출 모듈 비교**

| **모듈명** | **특징** | **처리 속도(초)** | **비고** |
| --- | --- | --- | --- |
| **PyMuPDF** | 빠르고 가벼움; 텍스트/이미지/메타데이터 추출 가능 | **1.06s** | 지속적 업데이트, 낮은 메모리 사용량 |
| pdfminer.six | 파이썬 기반 PDF 파싱; 정교한 텍스트 추출 | 14.42s | 상대적으로 느림, 메모리 사용 많음 |
| camelot | 표(Table) 추출 특화 | 0.5s | 표 중심, 일반 본문용에는 부적합 | - **PDF 처리 모듈: PyMuPDF** - **선정 이유** - 실험 결과 가장 **빠른 속도**(평균 1.06초/파일)와 **높은 정확도**를 보였으며, 대량 파일 처리 환경에 적합함 - **대량 문서 처리** 및 QA 기반 응답을 위한 **안정성**과 **범용성** 확보에 최적 - pdfminer.six는 정교한 파싱 강점이 있으나, 속도에서 크게 뒤처짐 - camelot등은 특정 목적 특화(표, 텍스트 등)로 범용성 부족

**HWP 문서 내 텍스트 추출 모듈 비교**

| **모듈명** | **특징** | **설치/운영 환경** | **비고** |
| --- | --- | --- | --- |
| **HWPLoader** | `LangChain` 연동용 `.hwp` → LangChain `Document` 변환; 청킹/파이프라인 최적화 | `langchain-teddynote` 패키지, `olefile`/`pyhwp`/`hwp5text` 활용 | `hwp-parser`보다 안정적이고 빠름; LangChain 프로젝트 특화; 단독 사용은 제한적 |
| hwp-parser | 오픈소스, 순수 Python 기반, 텍스트 파싱만 지원 | 간편한 설치(`pip install hwp-parser`) | 성능/정확도 낮음; 표/이미지 미지원; 테스트용·경량 프로젝트에 적합 |
| 한컴오피스 API | 상용 API, 표/이미지 포함 전체 콘텐츠 추출 가능 | 상용 라이선스 필요; Windows 환경; 별도 API 서버/SDK 필요 | 비용 발생; 상업적 제약 존재; 대규모 서비스에 적합 |
| python-hwpml | HWPML(XML) 포맷 전용; `.hwp` → `.hwpml` 변환 후 처리 | 추가 변환 툴 필요; `.hwpml`만 직접 처리 가능 | 일반 `.hwp` 바로 처리 불가; 변환 전용 워크플로우에 적합 | - **HWP 처리 모듈: HWPLoader** - **선정 이유** - HWPLoader는 **Langchain 연계 최적화**, **더 나은 성능과 안정성 제공** - Langchain 기반 RAG 파이프라인에서 청킹·벡터화까지 자연스럽게 연계 가능, 실제 활용성 가장 높음 - hwp-parser는 성능 및 정확도가 낮아 대규모 문서 처리 시 적합하지 않음 - 상용 라이선스(한컴오피스 API) 대비, 배포·유지 관리 부담이 적음

### **ii. Text Chunking**

> - RFP 문서를 의미 있는 **작은 정보 단위(Chunk)**로 나누어, 검색 효율과 생성 응답의 품질을 높이는 핵심 전처리 단계
> 

**Splitter 비교**

| **방식** | **질문 관련성** | **의미 단위 보존** | **중복 청크** | **컨텍스트 연결성** | **처리 속도** | **한계** |
| --- | --- | --- | --- | --- | --- | --- |
| **Section-Recursive** | **높음** | **높음** | **낮음** | **높음** | **빠름 - 0.16초** | **목차 의존성** |
| **Recursive** | 낮음~보통 | 낮음 | 보통 | 낮음 | 매우 빠름 - 0.13초 | 정보 단절, 잡음 |
| **Token** | 보통~높음 | 보통 | 높음 | 낮음 | 보통 - 0.45초 | 의미 분절, 중복 |

**기본 설정값** **및 선정 이유**

- **청크 사이즈 1000 tokens**
    - 1000 tokens은 제안서 한 **단락과 목차를 완결성 있게 표현** 가능
    - 500 tokens 이하일 경우 문맥 이해 부족 사례가 다수 발생
- **오버랩 250 tokens**
    - 문단 혹은 문맥 경계에서 **정보 단절 현상 방지**
    - Retriever가 **연관 문맥을 더 풍부하게** 검색할 수 있도록 지원

**평가 요약** 

- **Section-Recursive 방식**
    - 목차/구조를 기준으로 청크 분할
    - **질문 의도와 가장 밀접한 정보 제공**
    - ISP/제안서/정책 보고서처럼 구조화된 문서에 특히 효과적
- **Token 방식**
    - 분할 기준이 명확하고 범용성이 높지만, 문맥 단절로 생성 품질 저하 가능
- **Recursive 방식**
    - 구현이 가장 단순하지만, 잡음이 많고 맥락을 분리해버려 응답 품질에 악영향을 줌

**청크 사이즈 250 tokens, 오버랩 50 tokens 제한 실험** 

| **Splitter** | **청크 수** | **평균 길이** | **최소 길이** | **최대 길이** | **특징 요약** |
| --- | --- | --- | --- | --- | --- |
| **Recursive** | **205** | **245.38** | **65** | **249** | **적은 청크 수, 길이 거의 최대 제한 근접. 문맥 유지 좋음, 검색 정밀도 낮을 수 있음** |
| Token | 419 | 122.85 | 66 | 250 | 가장 많은 청크 수, 평균 길이 짧음. 정확한 검색에 유리, 문맥 파편화 우려 |
| Section | 208 | 236.82 | 47 | 250 | `Recursive`와 유사한 길이지만, 최솟값이 낮아 불균형 가능성 |

**청크 사이즈 1000 tokens, 오버랩 250 tokens 디폴트 실험**

| **Splitter** | **청크 수** | **평균 길이** | **최소 길이** | **최대 길이** | **특징 요약** |
| --- | --- | --- | --- | --- | --- |
| **Section** | **56** | **884.27** | **274** | **1000** | **문서 구조(제목/단락 등)를 반영하여 자연스러운 분할 가능. 정보 밀도와 검색 균형 사이에서 적절한 타협점 제공** |
| Recursive | 55 | 984.00 | 295 | 999 | 문맥 보존에 최적화된 방식. 청크 수가 적고 길이가 길어, 긴 응답에 유리하나 검색 키워드가 청크에 누락될 확률이 있음 |
| Token | 112 | 486.16 | 228 | 784 | 고르게 짧은 청크로 세분화되어 검색 정확도는 높지만, 문맥 연계가 약해 응답 일관성이 떨어질 수 있음 |

### **iii. Embedding Process**

> - 청크(문서 조각)를 의미 기반 벡터로 변환
> - 질의(Query)와 문서 간 의미 유사도를 계산해, 의미가 유사한 문장들이 가까운 벡터 공간에 위치하도록 매핑
> 

**Embedding 모델별 비교**

| **모델** | **5개 문서 처리 속도** | **100개 문서 처리 속도** | **문서 추출 정확도** |
| --- | --- | --- | --- |
| **`nlpai-lab/KoE5`** | **2.56초** | **12.86초** | **상** |
| `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 0.6초 | 10.87초 | 하 |
| `text-embedding-3-large` | 6.23초 | 13.77초 | 중 |

> ❓**Query** : 경기도일자리재단의 정보시스템 운영 사업 중 보안 기능 요구사항이 언급된 것이 있나요?

| **모델** | **쿼리에 따라 추출한 문서** | **비고** |
| --- | --- | --- |
| **`nlpai-lab/KoE5`** | **재단법인경기도일자리재단 2025년 통합접수시스템 운영 관련 문서** | **가장 정확함** |
| `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 인천일자리플랫폼 정보시스템 구축 ISP 수립용역, 대학재정정보시스템 고도화, 수협중앙회 수산물사이버직매장 시스템 재구축 ISMP | 관련 없음 |
| `text-embedding-3-large` | 대한상공회의소 기업 재생에너지 지원센터 홈페이지 개편, 을지대학교 비교과시스템 개발, 인천공항운영서비스 차세대 ERP시스템 구축 | 보안 기능 요구사항 관련 문서 추출 |

**모델별 설명 및 선정 이유**

- **Hugging Face: `nlpai-lab KoE5`**
    - 속도는 상대적으로 느리지만 **문서를 정확하게 추출**하기 때문에 선택
    - **한국어 RAG에 최적화**: KoE5는 국내 환경에 맞춘 문서 임베딩 성능을 보임
- **OpenAI: `text-embedding-3-large`**
    - **의미론적 유사도에 강함**: 다양한 주제와 문장 구조에서 뛰어난 표현력
    - 가용 자원의 제약으로 속도는 느리지만 성능 대비 **효율적인 선택지**로 고려 가능

---

## 3-2. Retriever System

### **i. Vector DB (FAISS vs. Chroma)**

> - 문서 Embedding 후, 유사도 기반 검색을 위한 벡터 저장소 역할 수행
> - Vector DB의 성능은 Embedding 모델, 저장 방식, DB 처리 로직에 따라 상이
> 

**Vector DB 생성 속도**

| **항목** | **전체 문서 (100개 기준)** | **선택 문서 (5개 기준)** |
| --- | --- | --- |
| **FAISS** | **최소: 14초 최대: 6분 51초** | **최소: <1초 최대: 15초** |
| Chroma | 최소: 1분 13초 최대: 7분 32초 | 최소: 2초 최대: 32초 | - **FAISS는 생성 속도 측면에서 일관되게 우수** - Chroma는 문서 수가 적더라도 생성 속도 편차가 크고 느림

**Vector DB 주요 기능 차이**

| **항목** | **동일 이름 Vector DB 처리** | **발생 시 대처** |
| --- | --- | --- |
| **FAISS** | **덮어쓰기 가능** | **불필요** |
| Chroma | 중복 저장 불가 → 충돌 발생 (사전 삭제 필요) | `shutil`을 통해 기존 DB 삭제 후 재생성 필요 |

**기술적 고려 사항**

- 사용자가 입력한 질문과 raw **문서 간 의미 유사도**를 기반으로 관련 문서를 선별한 뒤, 이를 Embedding으로 변환하여 벡터 DB를 구성
- 두 DB 모두 검색 정확도는 유사했으나, **FAISS는 생성 시간과 처리 안정성 측면에서 뛰어남**

**선정 사유 요약**

1. 실시간 응답 속도를 고려한 **사용자 경험(UX) 중심 설계**
2. 향후 **Hybrid Search + Re-ranking** 전략과의 **병행 최적화 가능성**
3. 반복적인 Vector DB 재구성 시 **운영 효율성 우수**

---

### **ii. Retriever**

> - Chunk로 구성된 Vector DB에서 사용자 질문과 관련성 있는 문서를 추출하는 역할
> 

> ❓**Query** : 고려대학교 차세대 포털 시스템 사업이랑 광주과학기술원의 학사 시스템 기능 개선 사업을 비교해 줄래?

**Retrieval 방식별 결과 (top-5 기준)**

| **Search type** | Re-ranking | **추출된 문서(top-5)** | **속도(초)** |
| --- | --- | --- | --- |
| **Hybrid** | **O** | **1. 고려대학교_차세대 포털….pdf, 청크: 5 2. 고려대학교_차세대 포털….pdf, 청크: 1 3. 서영대학교 산학협력단_...hwp, 청크: 2 4. 광주과학기술원_학사시….hwp, 청크: 2 5. 고려대학교_차세대 포털….pdf, 청크: 14** | **2.50s** |
| Hybrid | X | 1. 고려대학교_차세대 포털….pdf, 청크: 0 2. 고려대학교_차세대 포털….pdf, 청크: 1 3. 광주과학기술원_학사시….hwp, 청크: 1 4. 을지대학교_을지대학교….hwp, 청크: 5 5. 고려대학교_차세대 포털….pdf, 청크: 14 | 0.32s |
| Similarity | O | 1. 고려대학교_차세대 포털….pdf, 청크: 5 2. 고려대학교_차세대 포털….pdf, 청크: 1 3. 서영대학교 산학협력단_...hwp, 청크: 2 4. 광주과학기술원_학사시….hwp, 청크: 2 5. 고려대학교_차세대 포털….pdf, 청크: 14 | 1.93s |
| Similarity | X | 1. 고려대학교_차세대 포털….pdf, 청크: 0 2. 을지대학교_을지대학교….hwp, 청크: 5 3. 고려대학교_차세대 포털….pdf, 청크: 1 4. 고려대학교_차세대 포털….pdf, 청크: 14 5. 을지대학교_을지대학교….hwp, 청크: 1 | 0.28s |

**Retrieval 방식 설명**

- **Similarity search**
    
    → 사용자 질문과 각 문서 청크 간의 의미 유사도를 계산하여 **가장 유사한 청크** 반환
    
- **Hybrid search**
    
    → **Similarity + BM25**(문자 기반 키워드 검색) 조합
    
    벡터 기반의 의미 유사도와 텍스트 기반 검색을 **가중 조합**하여 더 정교한 결과 도출
    
- **Re-ranking 적용**
    
    → Similarity 또는 Hybrid 검색을 통해 검출된 문서 청크와 질문 간 **의미 연관성 재평가** 
    
    가장 관련성이 높은 상위 `top_k` 청크를 다시 정렬해 반환
    

**실험 결과**

- 상위 5개의 문서만 비교 시 단순 Similarity Search는 **광주과학기술원 관련 문서**를 상위로 끌어오지 못함
- 반면 **Hybrid Search**는 두 기관 관련 문서를 모두 정확히 추출
- 두 가지 방식 모두 **Re-ranking**을 함께 적용하면 **검색 결과의 품질이 안정적으로 향상**
- 따라서 속도가 조금 느리더라도 **Hybrid search +Re-ranking** 조합이 **정확도와 포괄성 측면에서 최적**

## 3-3. Answer Generation

### **i. Response Generator**

> - `Retriever` 과정에서 추출된 문서 청크들과 사용자 질문을 조합한 `Prompt` 를 입력받아, 실제 사용자가 이해할 수 있는 자연어 응답을 생성하는 기능
> 

**실험 모델 비교**

| **모델명** | **VRAM 사용량** | **응답 속도** | **응답 품질** | **비고** |
| --- | --- | --- | --- | --- |
| **Markr-AI/Gukbap-Qwen2.5-7B** | **14.7GB** | **141.42s** | **상** | **RAG ✅ P.E. ✅** |
| microsoft/Phi-4-mini-instruct | 7.2GB | 89.12s | 상 | 영어 답변 |
| Bllossom/llama-3.2-Korean-Bllossom-AICA-5B  | 10.7GB | 167.72s | 상 | RAG ❌ P.E. ❌ |
| Junmai/KIT-5.8b | 22.7GB | 190.64s | 하 | 답변 ❌ |

**모델 분석 - Hugging Face**

- **`Markr-AI/Gukbap-Qwen2.5-7B`**
    - **파라미터 수**: 7B
    - **선정 사유**: 다양한 Hugging Face 모델을 실험한 결과, 유일하게 **RAG 기능**과 **Prompt Engineering(P.E.)**이 모두 제대로 작동하면서도 높은 응답 품질을 유지한 모델.
    - **한계점**: 다소 느린 응답 속도(140초 이상)에도 불구하고, **정보의 정합성**과 **표현의 자연스러움** 측면에서 우수하여 최종 채택
- **`microsoft/Phi-4-mini-instruct`**
    - 장점: 빠른 속도와 안정성
    - 단점: 영어로 응답을 생성하는 경우 다수, RAG나 한국어 Prompt가 정상적으로 반영되지 않아 실사용에는 부적합
- **`Bllossom/llama-3.2-Korean-Bllossom-AICA-5B`**
    - 장점: 한국어 표현력이 우수하고 답변 품질도 높은 편
    - 단점: RAG 미작동, P.E. 효과 없음 → 실제 문서 기반 응답 불가능
- **`Junmai/KIT-5.8b`**외 모델들
    - 대부분의 쿼리에서 응답 자체를 생성하지 못해 실사용 불가

**모델 분석 - OpenAI**

- **`gpt-4.1-nano`**
    - **파라미터 수**: 비공개
    - **선정 사유**: 경량화된 최신 GPT 계열 모델 중 하나로, 제한된 자원 내에서도 **자연스러운 한국어 RAG 응답**을 생성
    - **특징**: **Prompt Engineering 반영**이 우수, 응답 속도는 중간 정도이나 **답변 품질은 안정적**

**선정 모델 응답 예시**

> ❓**Query : 한국 원자력 연구원에서 선량 평가 시스템 고도화 사업을 발주했는데, 이 사업이 왜 추진되는지 목적을 알려 줘.**

- **Hugging Face: `Gukbap-Qwen2.5-7B`**

> 🗨️ 한국 원자력 연구원에서는 선량 평가 시스템 고도화 사업을 통해 다음과 같은 목적을 달성하려 합니다:
>
> 1. **규제 준수**: 원자력안전위원회의 규정에 맞춰 연간 선량의 준수 여부를 확인할 수 있는 체계를 구축합니다.
> 2. **평가 기준 개선**: ICRP 60 기반의 평가장을 장기적으로 개선하여 더 정확한 선량 평가가 가능하도록 합니다.
> 3. **액체 유출물 피폭 평가**: 액체 유출물에 의해 생기는 선량 평가 기능을 추가하여 다양한 방식의 선량 평가가 가능하도록 합니다.
> 4. **업무 생산성 향상**: UI 개선을 통해 업무 처리 속도를 높이고, 신속한 의사 결정 환경을 마련하여 생산성을 증진시키는 목표로 합니다.
>
> [출처: 한국원자력연구원_한국원자력연구원 선량평가시스템 고도화.hwp, 청크번호: 0, 1]

- **OpenAI**: **`gpt-4.1-nano`**

> 🗨️ 이 사업은 원자력안전위원회 고시 제2019-10호와 ICRP 60 기반 평가장기 개선, 액체유출물에 의한 주민피폭선량평가 필요성에 따라 규제요건을 준수하고, 업무생산성을 향상시키기 위해 추진됩니다. 또한, 선량평가 관련 규제 수요에 대응하고 데이터 신뢰성 및 생산성 향상을 목표로 합니다. 
> [출처: '한국원자력연구원 선량평가시스템 고도화', 청크번호: 0]

---

## 3-4. Chat Memory

### **i. Chat History**

> - 사용자의 **이전 대화 내용**을 기억하거나 참조하여 **문맥을 유지**하고 **일관성 있는 응답**을 생성할 수 있게 하는 기능
> 

**Chat History 저장 및 활용 방식**

- **대화 기록 저장 위치**: `st.session_state.chat_history`
- **업데이트 시점**: 질문-응답이 종료될 때마다 `Config` 객체에 기록
- **활용 시점**: 다음 질문 처리 시 `RAG_Pipeline`에 반영하여 검색 및 응답 품질 향상

**Chat History 적용 순서**

| 단계 | 적용 위치 | 반영 방식 |
| --- | --- | --- |
| 1️⃣ | **Data Loader** | 요약된 이전 대화 내용을 **질문(Query)에 포함**하여 검색 |
| 2️⃣ | **Retriever** | 유사도 검색 시 **요약된 이전 대화 내용 포함** |
| 3️⃣ | **Generator** | 프롬프트에 `이전 대화:{chat_history_section}` 형식으로 삽입 |

**Chat History 적용 예시**

**대화 내역**

- **role - User**
    
    > ❓이 사업에서 제안된 디지털트윈 기반 데이터 수집 방식은 어떤 절차를 따르나요?
    
- **role - Ai**
    
    > 💡해당 사업에서는 디지털트윈 구현을 위해 다음과 같은 데이터 수집 절차가 명시되어 있습니다: 
    >
    >     1) 현장 계측 센서 및 IoT 기기를 활용한 실시간 데이터 수집, 
    >     2) 기존 시스템과의 연계를 통한 이력 데이터 확보, 
    >     3) 수집된 데이터를 공통 데이터 허브로 전송 및 통합, 
    >     4) 표준화된 포맷으로 전처리 후 디지털트윈 플랫폼에 반영합니다.
    

**요약된 대화 내역**

- **summary**
    
    > 📝『국가철도공단 디지털트윈 ISP 수립 용역』 제안요청서 3.2항에는 '데이터 수집 및 처리 방식'이 구체적으로 기술되어 있으며, 
    >     센서 기반 실시간 수집과 플랫폼 통합 절차가 중심 내용으로 구성되어 있습니다. 특히 BIM 기반 통합 구조와의 연계가 강조됩니다.
    

**Chat Memory 도입 효과 및 한계**

- **Chat Memory 기능 도입** 후 다중 질의 시의 **응답 일관성**과 **문맥 반영률**이 크게 향상
- 다만 불필요한 과거 정보 반복 반영 등의 이슈가 발생할 수 있음

---

# **4.🖱️ UI 및 사용자 경험**

## 4-1. 초기 화면 구성
## 4-2. 각종 기능 소개

### i. 문서 요약 및 분석
`.env` 파일이 없을 경우
경고문이 나오지만 실행에는 문제 없음

### ii. API_KEY가 없을 경우
`API_KEY` 가 없을 경우 경고문 + 입력란 생성

### iii. 초기화, 리셋, 리로드 버튼

**Vector DB 초기화 버튼**

- DB 파일들은 설정에 따라 파일명이 유동적이게 변경
- 설정에 따라 파일명을 추적하여 해당 DB를 제거하는 버튼

**예시**

초기화 버튼 적용 후 DB 삭제

**리셋 버튼**

- 질의응답을 거치게 되면 `session_state` 그리고 `config`의 `chat_history`에 기록이 저장됨
- 이 기록이 후속 질문에 영향을 준다고 판단할 경우 사용자가 직접 히스토리를 비우고 새롭게 질문을 시작할 수 있도록 만든 버튼

문서 출력 화면

대화 내역 리셋 적용

**모델 리로드**

- 모델을 변경할 경우, 캐시에 저장된 모델을 해제하는 버튼
> 📱 ### ⅳ. 모바일 지원

---

# **5.⚡ Fast API 연동**

## 5-1. API 흐름도
![report2](/asset/images/projects/codeit/rfp-rag/star2.png)
> Frontend–Backend Interaction Diagram
> 

## 5-2. 서비스 운영

### **i. GCP(Google Cloud Platform) 기반 VM 환경**

- **실행 환경**: Google Cloud Platform(GCP)의 Ubuntu 기반 가상 머신(VM)
- **운영체제**: Python 3.10 환경의 Ubuntu 서버에서 가상환경(venv) 기반으로 프로젝트 실행
- **네트워크 설정**: 고정 외부 IP를 할당하고, TCP 8100 포트 등을 방화벽에서 허용하여 외부 접근 가능
- **접근 구조**: Streamlit 프론트엔드가 FastAPI의 `/chat` 엔드포인트에 API 요청

### **ii. FastAPI 실행 환경**

- **프레임워크**: 비동기(ASGI) 기반의 Python 웹 프레임워크 FastAPI
- **서버 구성**
    - **gunicorn**: 멀티 프로세스 기반 WSGI 서버, 프로덕션 환경에서 요청 분산 처리 수행
    - **UvicornWorker**: gunicorn과 연동되어 FastAPI의 비동기 엔드포인트를 처리
- **실행 명령어**
    
    ```markdown
    gunicorn backend.main:app -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8100 --workers 4
    ```
    

---

# **6. 📈 개발 이슈 및 해결 방안**

## 6-1. **Problem Solving**

### **i. Settings**

**설정 관리의 어려움**

- **문제**: 개발이 진행될수록 설정값이 많아져서 관리하기 어려움
- **해결**: `config.yaml`로 통합하여 관리

**JupyterLab 숨김파일 접근 문제**

- **문제**: 가상환경 내부의 숨김파일 접근이 불가능함
- **해결**: JupyterLab Hidden File Visibility 설정을 터미널 커맨드로 덮어씌워 이용해 극복

**통합 실행 파일`main.py`의 혼란**

- **문제**: 분업으로 인한 통합 실행 파일의 모호함
- **해결**: 기능별 `main.py`와 실제 `main.py`를 만들어 실행진입점에서만 실행

**로깅 부재로 피드백 불가**

- **문제**: 로깅이 되지 않아 결과에 대한 피드백이 제대로 되지 않음
- **해결**: `langsmith`를 사용해 설정과 기능별 결과를 로깅하여 latency 측정

 **에러 발생 시 추적 어려움**

- **문제**: 코드 에러가 나는 경우 정확히 어느 코드의 어떤 부분인지 찾기 어려움
- **해결**: 방어적 로직을 구현해 에러 메시지 양식(`folder.module.function`)을 통일

**개발 단계 디버깅 문제**

- **문제**: CLI 실행 시 중간 과정 출력이 없어 디버깅이 어려움
- **해결**: Verbose 옵션을 추가해 단계별 실행 흐름을 출력하여 개선

**모델 캐시 및 리소스 관리 문제**

- **문제**: 다양한 모델 실험으로 캐시가 과도하게 누적되고 VRAM 사용량 증가로 실행 중단
- **해결 1**:
    1. 공유 캐시 경로를 설정해 중복 다운로드 방지
        
        → 경로 충돌 문제 발생
        
    2. 충돌 시 캐시 경로 재설정 및 가상환경 초기화로 문제 해결
- **해결 2**:
    1. 주기적인 캐시 정리와 모델 로딩 구조 개선으로 VRAM 사용 최적화

**Streamlit 페이지 질문 병렬 처리 불가**

- **문제**: 다수의 사용자를 상정, 팀원 여러 명이 접속하여 동시 질문을 할 경우 터미널 내부에서 충돌
- **해결:**
    1. GCP 환경에서 방화벽 설정에서 TCP 포트를 개인마다 할당하여 개인 페이지를 열어 임시 해결
    2. Fast API 등을 이용해 다수 사용자 질문에 대한 병렬 처리 방법을 도입

**모듈 인자 일관성 부족으로 코드 관리 어려움**

- **문제**: 개발이 진행될수록 모듈이 받는 인자가 들쭉날쭉하고 코드가 최적화되지 않음
- **해결**: 팀 전체 코드 리뷰 시간을 갖고 리팩토링 규칙을 세워 정리

**실행 환경 재현 어려움**

- **문제**: 로컬 환경 차이로 패키지 버전 불일치 및 실행 오류 발생 가능성 확인
- **해결**: 실행 환경 전체 구성을 반영한 `environment.yaml`을 작성해 정확한 재현이 가능한 환경 구성

### **ii. Data Loader**

**문서 확장자가 일관되지 않음**

- **문제**: hwp 파일과 pdf 파일이 혼재되어 있음
- **해결**: 처음에는 pdf 관련 모듈이 다양하고 효율적이므로 hwp를 pdf로 바꿈
    
    

**한글 미설치**

- **문제**: hwp를 pdf로 바꿀 때 한글이 설치되어 있어야 함
- **해결**: 적절한 reader 모듈(`PyMuPDF`, `HWPLoader`)을 찾아 두 가지 파일 타입 모두 처리

**PDF 파일 내부의 이미지 문제**

- **문제**: 파일에 이미지가 있는 경우가 드물고 양이 적으며 내용이 중요하지 않음
- **해결**: OCR 적용 여부 선택지를 포함해 파일 내부 이미지를 처리함

**PDF parser 경고 메시지** 

- **문제**: 하나의 문서에서 PyMuPDF의 경고 메시지가 지속적으로 발생
- **해결**: MuPDF의 공식 문서 기준, 자체적으로 syntax error를 수정하며 기능상 문제가 없음을 확인

**Token/Recursive 분할의 한계점**

- **문제**: 기존의 Token/Recursive 방식은 청크 크기 제약으로 문맥이 단절됨
- **해결**: 의미 기반 분할 방식(Section+Recursive)을 추가하고 청크 크기를 확대하여 개선함

**적절한 의미 분절 단위 선정의 어려움**

- **문제**: 문서의 어디부터 어디까지가 적절한 크기의 청크 단위인지 모호함
- **해결**: 문서마다 단락이 바뀌는 지점을 파악하여 기준으로 채택해 분리

**청크 길이 불균형 문제**

- **문제**: 의미 단위로 문서를 분할하다 보니 청크 길이가 너무 짧거나 긴 경우가 존재
- **해결**: 짦은 경우는 청크를 이어 붙이고, 긴 경우는 일부를 잘라 다음 청크와 이어 붙여서 해결

**단락 구분 어려움**

- **문제**: 기존의 문서 방식으로는 정확한 단락 머리 표를 구분하지 못함
- **해결**: 다양한 문단 분리 기호를 추가하여 정확하게 의미 단위를 분리

**유사도 계산 시 벡터 타입 불일치 문제**

- **문제**: 질문과 유사한 문서를 검색하는 경우 임베딩 모델에 따라 반환되는 벡터 타입이 달라 호환성 오류 발생
- **해결**: 임베딩의 encode() 메서드를 기준으로 분기 처리하여 torch.Tensor()에는 cos_sim(), numpy.ndarray에는 cosine_similarity()를 적용

**청크 생성 실패 시 예외 처리 부재**

- **문제**: 특정 문서에서 스플리터가 청크를 생성하지 못한 경우, 빈 리스트가 반환되어 오류 발생
- **해결**: 청크 반환 결과를 검증하고, 빈 경우 기본 청크 방식으로 폴백 처리하여 안정성 확보

### **iii. Embedding**

**Default Embedding 모델 선정 문제**

- **문제**: 다양한 임베딩 모델이 존재함에 따라 선정에 어려움을 겪음
- **해결**: 뚜렷한 차이점이 존재하지 않는다면 목표에 가장 적합한 것을 선택

**Embedding 모델마다 할당된 차원 수가 고정**

- **문제**: 임베딩 차원 자동 추론 실패에 따른 FAISS Index 초기화 오류
- **해결**: 모델별 차원을 명시적으로 설정하거나, 차원을 계산해서 동적으로 할당

**임베딩 모델 불일치로 메모리 과다 사용**

- **문제**: 여러 모듈에서 사용되는 임베딩 모델이 제각각 달라서 메모리를 과도하게 차지함
- **해결**: 통일된 임베딩 모델로 메모리 사용량을 줄이고 통일된 결과로 신뢰도 향상

### **iv. Vector DB**

**청크 임베딩 시 토큰 수 초과 문제**

- **문제**: 문서를 청크 단위로 임베딩 시, 일부 모델은 입력 토큰 수 한계를 초과함
- **해결 1**:
    1. Chunk_size를 축소하여 진행
        
        → 문맥이 끊어져서 출력되는 경우가 다수 존재하여 정보 손실이 증가
        
    2. 다양한 splitter 방식을 도입함으로써 비교적 작은 토큰 수로도 문맥 정보의 유지가 가능
- **해결 2**:
    1. `batch` 단위로 청크를 나누어 임베딩 및 DB 저장해서 메모리 사용량 분산 및 처리 안정성 향상
    전체 문서 → 9,043개 청크 → 71개 배치 → 순차적 Vector DB 저장
    2. 질문과 유사도 계산을 선행 단계에서 추가 진행하는 것으로 관계 문서만 불러오기 (소규모 DB 재생성)

**Chroma 기반 DB 저장 충돌**

- **문제:** Chroma는 동일 이름의 DB를 중복 저장할 수 없으며, 아래와 같은 충돌 오류가 발생
- **해결:** 기존 디렉터리를 `shutil` 등으로 삭제한 후 재생성

**질문별 벡터 DB 재생성으로 인해 속도 저하**

- **문제**: 질문마다 전체 문서를 기반으로 한 새로운 벡터 DB를 생성해 실험 속도가 느려짐
- **해결 1:** 전체 문서를 DB에 저장할 경우, 이후 저장 과정을 무시하는 로직 추가
- **해결 2:** 질문과의 유사도를 계산해 상위 N개의 문서만 불러오는 방식으로 개선
    
    

### **v. Retriever**

**Similarity Search의 한계점**

- **문제**: Similarity search는 속도는 빠르지만 검색된 문서가 부정확함
- **해결**: Hybrid search와 Re-Ranking을 통해 속도는 속도를 다소 희생하여 성능을 상향시키는 방식을 도입

**질문과 청크 간 유사도 계산의 낮은 정확성**

- **문제**: 단순 코사인 유사도 계산 방식은 질문과 문서 청크 간의 미묘한 의미 차이를 반영하지 못해, 연관성이 낮은 청크가 상위에 랭크되는 한계가 있음
- **해결**: `cross-encoder/ms-marco-MiniLM-L-6-v2` 모델을 도입하여 질문과 청크를 함께 입력받아 유사도를 직접 예측함으로써, 코사인 계산보다 훨씬 정밀한 의미 기반 유사도 평가가 가능해짐

**문서 기반 질의응답의 정밀도 부족**

- **문제**: 여러 문서가 연관된 질문에서 특정 문서에만 편중된 청크를 반환하여 정보 누락 발생
- **해결**:
    1. Top-k 검색으로 후보를 줄이고 Re-ranking를 적용한 다음 다시 Top-k 선택하여 개선
        
        → 그럼에도 여전히 관련된 문서를 가져오지 못하는 경우 발생
        
    2.  벡터 DB에 저장된 문서마다 최소 청크 수를 보장하는 로직으로 모든 문서를 가져옴
        
        → 제너레이터에 너무 많은 청크를 보내 메모리를 과다하게 차지함
        
    3. 임베딩 모델의 성능을 올려 최소 청크 수를 보장하는 로직을 없애고 기존 로직으로 복귀

### **vi. Generator**

**HuggingFace 모델에 따른 GPU VRAM 부족**

- **문제**: 허깅페이스 모델의 크기로 인해 모델의 추론 속도 지연 및 메모리 초과 발생
- **해결**: 모델 양자화를 적용해 메모리 사용량을 줄이고 다양한 경량 모델을 실험하여 최적화

**모듈 구조의 복잡성으로 인한 유지보수 어려움**

- **문제**: OpenAI와 HuggingFace 모델이 하나의 모듈에서 처리되어 복잡한 구조를 가짐
- **해결**: 각각의 모델을 별도의 모듈로 분리하여 유지보수 용이함

**모델이 프롬프트 지시에 불응**

- **문제**: 최초 프롬프트를 적용했을 때 모델이 규칙을 위반하는 경우가 잦음
- **해결**: 규칙을 엄격하게 설정하고 위반할 때 페널티를 추가하여 개선된 프롬프트 적용

**모델 성능의 전반적인 한계**

- **문제**: 허깅페이스 모델의 답변 생성력, RAG 성능 및 prompt 이해력 부족
- **해결**: 더욱 다양한 모델을 실험해서 최적의 모델을 찾아 적용함

### vii. Chat History

**대화 맥락 파악 어려움**

- **문제**: 단일 질의응답만 가능해 대화 흐름을 유지하여 추가적인 질문에 답변이 불가함
- **해결**: chat history를 만들어 이전 대화 흐름을 요약해 다음 질문에 추가하여 해결

**유사도 검색 시 맥락 반영 어려움**

- **문제**: Top_k개의 문서를 불러올 때 맥락을 무시하고 질문에만 의존해 불러오는 문제
- **해결**: 질문 Query에 이전 대화 요약을 함께 넣어줌

### viii. User Test

**Streamlit `text_input()` 메서드의 한계**

- **문제**: 초기 UI에서 `text_input()`을 이용해 사용자 질문을 입력하는 방식을 사용했으나, 이전 질문이 계속 입력 창에 남아있으며 입력 창에 글이 있는 상태로 설정을 바꿀 경우 자동으로 검색을 진행함
- **해결**: 버튼을 설정하여, 버튼을 누르지 않으면 검색이 진행하지 않도록 수정, `Enter` 키와 버튼을 연동시켜 UX 편리성 강화
    
    

**대화 기록 UI 문제**

- **문제**: 이전 질문이 계속 입력 창에 남아있으며 일일이 지워야 한다는 점에서 비효율성을 확인
- **해결**:
    1. 파이프라인 마지막 단계에 초기화 구문으로 `session_state` 초기화 시도
        
        → 렌더링 이후의 수정은 불가능하다는 것을 알게 됨
        
    2.  `key`값을 `query_key`로 설정 `value`를 `0`부터 시작하여 질문을 받을 때마다 증가를 시키는 것으로 새로운 키를 할당하며 렌더링 이후 key 값이 변경되어 리셋이 동작할 수 있도록 적용
        
        → 초기화 과정만으로 기능이 너무 복잡한 모습으로 변함
        
    3. `chat_input()` 메서드의 존재 확인 및 도입으로 자동 해결

**동시 추론이 일어날 때 생기는 주요 문제**

- **문제**: 한 사용자의 LLM 추론이 10초 걸린다면, 다른 사용자는 그 10초 동안 대기 상태가 됨. 리소스를 많이 쓰는 모델의 경우, 시스템이 과부하되어 느려지거나 중단될 수 있음
- **해결**: 추론 로직을 Fast API로 따로 만들어 Streamlit은 API만 호출

**허깅페이스 모델 반복 로딩**

- **문제:** 허깅페이스 모델 로드 시, 분할된 가중치를 불러오기에 1~2분이 소요됨. 질문마다 모델을 반복 로드해 불필요한 지연이 발생함
- **해결:** `@cache_resource` 데코레이터를 활용해 모델을 한 번만 로드하고 캐시에 저장, 반복 로드를 방지하여 효율성을 높임

**OpenAI 모델 이용 시 API KEY 없을 경우 에러**

- **문제:** OpenAI로 답변 생성을 시도할 때, `.env` 파일이 없을 경우 진행 불가
- **해결:** UI로 상태 표시를 하여, 사용자가 직접 입력할 수 있게 적용

**모델에 따른 답변 생성 지연 시간으로 인한 사용자의 혼란**

- **문제**: HuggingFace 모델의 경우 답변 생성에 최소 40초 그리고 최대 3분가량이 소요되는 경우가 있으나 사용자가 확인할 방법이 없어, 시스템이 멈춘 것은 아닌지 혼란이 생길 수 있다.
- **해결**: `spinner` 등으로 임베딩 생성 과정, DB 생성 과정, 문서 추출과 답변 생성 과정을 로딩으로 보여줌. 그러나 Fast API 연결에서 개별적 로딩 출력에 어려움이 있었기에, 실행 과정을 파이프라인으로 묶어 단 하나의 로딩으로 정리함

### ix. Frontend (UI)

**사용자가 설정을 바꿀 수 없음**

- **문제**: 설정 고정 적용 시 사용자가 선택할 수 있는 옵션이 없음
- **해결**: UI에 `slider` , `toggle` 등을 적용해 바뀐 값을 `config` 에 적용하는 방식을 채택
최종 파이프라인은 사용자가 옵션 설정에서 바꾼 설정을 config에 저장해 최종 실행

**사용자 질의응답이 UI에 남지 않음**

- **문제**: 출력 문서 출력 위치의 최적화
- **해결**:
    1. 질문과 대답을 히스토리에 저장 후 질문마다 끝날 때 히스토리를 출력
        
        → 입력 위치에 따른 출력물 상태의 어색함
        
    2. 대화 입력 창의 위치에 따른 출력물 방향 설정 (현재 대화 입력 창은 상단에 위치하는데 역방향으로 출력됨)
        
        → 대화 내역이 길어질수록 리랜더링에 들어가는 자원 소모량이 늘어남
        
    3. 최대 길이를 정하여 출력 히스토리의 최댓값을 20으로 고정

**모델 응답에 참고한 문서의 위치 선정 문제**

- **문제**: 메타 데이터 및 `Retriever`에서 문맥 정보를 보여주는 문서 정보 출력의 위치 설정 문제
- **해결**: 
    1. 초기 UI에서는 단일 질문만을 가능한 설정이었으므로 상단에서 입력에 질문을 넣게 되면 하단에 문서 정보와 답변을 보여주는 형식 적용
        
        → 채팅 형식으로 바뀌며 문서 출력의 위치가 대화 기능에 방해되는 것처럼 보임
        
    2. 사이드 바에 페이지를 만들어 사용자가 직접 버튼을 눌러 사이드 바가 옵션과 추출 문서 내역을 보여주는 방식으로 적용
        
        → 추출 문서 내역이 자동 갱신하는 부분에 있어 추가 초기화 과정이 필요함
        → 즉각적으로 갱신되는 것에 있어 느린 모습
        
    3. 메인 화면에 탭으로 두어 `session_state`를 업데이트하는 것으로 수정

**모델에 따른 답변 생성 지연 시간으로 인한 사용자의 혼란**

- **문제**: 허깅페이스 모델의 경우 답변 생성에 최소 40초 그리고 최대 3분가량이 소요되는 경우가 있으나 사용자가 확인할 방법이 없어, 시스템이 멈춘 것은 아닌지 혼란이 생길 수 있음
- **해결**:
    1.  `spinner` 등으로 임베딩 생성 과정, DB 생성 과정, 문서 추출과 답변 생성 과정을 시각화
        
        → Fast API 연결에서 개별적 로딩 출력에 어려움 
        
    2. 실행 과정을 파이프라인으로 묶어 단 하나의 로딩으로 정리

### **x. Backend**

**여러 사용자의 동시 질의응답 불가**

- **문제**: uvicorn은 단일 프로세스로 동시에 여러 요청이 들어오면 처리에 한계를 보임
- **해결**: gunicorn으로 다중 worker를 실행해 병렬 처리가 가능하도록 개선

**Streamlit 내 분산된 RAG API 호출**

- **문제**: Streamlit에서 `spinner`로 RAG 단계별 처리 시, API 관리가 복잡해짐
- **해결**: 단일 API로 pipeline을 통합해 `spinner` 내부에서 한 번에 처리되도록 간소화

**모든 API 요청이 동일한 config 객체 공유**

- **문제**: 여러 Streamlit 탭에서 동시에 질문할 경우 응답이 서로 뒤섞임
- **해결**: 요청마다 `deepcopy(config)`로 독립된 config 사본을 사용하도록 변경

---

# **7. 📌 결론**

**프로젝트 목표 달성 여부**

본 RFPilot 프로젝트는 B2G 컨설팅 스타트업을 위해 제안요청서(RFP) 문서를 실시간으로 처리하는 챗봇 시스템을 성공적으로 개발했습니다. Retrieval-Augmented Generation(RAG) 기술을 활용하여 RFP 문서를 빠르게 요약하고 사용자의 질문에 실시간으로 응답함으로써 컨설턴트가 긴 문서를 검토하는 시간을 단축하고 전략적 컨설팅에 집중할 수 있도록 지원합니다. 이 시스템은 고급 NLP 모델, 벡터 데이터베이스, 사용자 친화적 인터페이스를 통합하여 정확하고 효율적인 응답을 제공합니다.

- **효율적인 문서 처리**: PDF와 HWP 파일 형식을 모두 처리할 수 있는 데이터 로더를 구현하여, 혼합된 파일 형식과 문서 내 이미지 처리 문제를 해결했습니다. PyMuPDF와 HWPLoader를 활용해 한글 소프트웨어 없이도 다양한 문서 형식을 처리할 수 있도록 했습니다.
- **확장 가능한 벡터 데이터베이스**: FAISS와 Chroma를 사용한 임베딩 및 벡터 데이터베이스 파이프라인을 개발하여 문서 임베딩의 효율적인 저장과 검색을 가능하게 했습니다.
- **정확한 검색 시스템**: 유사도 검색과 Re-Ranking을 결합한 Hybrid 검색 방식을 도입하여 검색된 문서의 정확도를 높였으며, 성능과 속도 간 적절한 균형을 유지했습니다.
- **응답 생성**: `KoE5`, `OpenAI text-embedding-3-large`, `Gukbap-Qwen2.5-7B`, `OpenAI GPT-4.1-nano` 등 다양한 언어 모델을 통합하여 문맥에 맞는 응답을 생성했습니다. 프롬프트 엔지니어링을 최적화하여 모델의 이해도와 응답 품질을 개선했습니다.
- **UI, UX 개선**: Streamlit 기반 프론트엔드를 구축하고 `@cache_resource` 데코레이터를 활용해 허깅페이스 모델 로딩 시간을 질문당 로딩을 단일 로딩으로 줄였습니다. API 키 유효성 검사에 대한 UI 피드백을 추가해 사용자 경험(UX)을 개선했습니다.
- **원활한 백엔드 통합**: FastAPI 기반 백엔드를 배포하여 API 기반 상호작용을 지원하고, 벡터 데이터베이스와의 실시간 쿼리 처리를 통해 확장성을 확보했습니다.
- **프로젝트 관리 및 협업**: 2025년 5월 12일부터 5월 30일까지 계획된 타임라인 내에서 데이터 수집, 모델 구현, UI 개발, 코드 리팩토링 등 모든 마일스톤을 성공적으로 완료했고, GitHub, Notion, Discord를 활용해 협업을 효율적으로 관리했습니다.

**한계점 및 개선 방향**

- **문서 처리 방식 개선 필요**
    - 한계점: 일부 문서에서 비정형 구조, 표나 이미지로 구성된 정보는 추출 정확도가 떨어졌으며, OCR 등의 후속 처리가 필요할 수 있습니다. 또한, Streamlit 웹 UI에서 새로운 문서에 대응하지 못한다는 점이 존재합니다
    - 개선 방향: 시각 정보와 언어 정보를 동시에 처리할 수 있는 VLM(Vision Language Model)을 도입하여 OCR 부분을 처리하는 것을 고려할 수 있습니다. 다만, 적용하려는 VLM 모델의 학습 정도에 따라 처리할 수 있는 데이터의 한계가 존재해, 데이터의 복잡도에 따라 추가적인 학습의 필요성이 요구될 수 있습니다.
- **RAG의 구조적 한계**
    - 한계점: 일부 사전 학습된 모델은 비교적 정확한 응답을 생성했으나, Prompt의 맥락을 충분히 이해하지 못하는 경우도 있었습니다.
    - 개선 방향: 충분한 학습을 거친 모델에 추가 학습을 진행하되 프롬프트에 대한 이해도를 높이기 위해 instruction-tuning을 진행하는 것을 고려할 수 있습니다. 다만, 선정된 모델의 크기에 따라 학습데이터의 양 또한 크게 늘어날 수 있음을 인지해야 할 것입니다.
- **질문 해석의 한계**
    - 한계점: 줄임말, 모호한 표현이 포함된 질문에 대해 적절한 문서 추출이 어려웠습니다.
    - 개선 방향: 이를 보완하기 위해 질문 전처리 또는 질의 정제 알고리즘 도입이 고려될 수 있습니다.
- **자원 제약으로 인한 백엔드 처리 한계**
    - 한계점: Docker 컨테이너화 및 HTTPS 보안 적용을 계획했으나, VM 서버의 저장 공간 부족으로 인해 실제 적용은 보류되었습니다.
    - 개선 방향: 추후 충분한 리소스를 갖춘 서버 환경에서 보안 및 배포 자동화를 적용해 볼 수 있을 것입니다.

---

**이 보고서는 프로젝트 수행 중 작성된 실험 로그, 협업 일지, 회의 기록 등을 바탕으로 구성되었습니다.**

---
